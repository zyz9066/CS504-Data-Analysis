{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Machine Learning Classifiers\n",
    "## 6.3 Random Forest model\n",
    "### Read in & clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "data = pd.read_csv('SMSSpamCollection', sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(' ')), 3)*100\n",
    "\n",
    "def count_cap(text):\n",
    "    count = len(re.findall('[A-Z]', text))\n",
    "    return round(count/(len(text) - text.count(' ')), 3)*100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(' '))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "data['cap%'] = data['body_text'].apply(lambda x: count_cap(x))\n",
    "\n",
    "def clean_text_ps(text):\n",
    "    text = ''.join([char.lower() for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    tokens = re.findall('\\w+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "def clean_text_wn(text):\n",
    "    text = ''.join([char.lower() for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    tokens = re.findall('\\w+', text)\n",
    "    text = [wn.lemmatize(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# TF-IDF using stemming\n",
    "tfidf_vect_ps = TfidfVectorizer(analyzer=clean_text_ps)\n",
    "X_tfidf_ps = tfidf_vect_ps.fit_transform(data['body_text'])\n",
    "X_tfidf_feat_ps = pd.concat([data['body_len'], data['punct%'], data['cap%'], pd.DataFrame(X_tfidf_ps.toarray())], axis=1)\n",
    "\n",
    "# TF-IDF using lemmatizing\n",
    "tfidf_vect_wn = TfidfVectorizer(analyzer=clean_text_wn)\n",
    "X_tfidf_wn = tfidf_vect_wn.fit_transform(data['body_text'])\n",
    "X_tfidf_feat_wn = pd.concat([data['body_len'], data['punct%'], data['cap%'], pd.DataFrame(X_tfidf_wn.toarray())], axis=1)\n",
    "\n",
    "# CountVectorizer using stemming\n",
    "count_vect_ps = CountVectorizer(analyzer=clean_text_ps)\n",
    "X_count_ps = count_vect_ps.fit_transform(data['body_text'])\n",
    "X_count_feat_ps = pd.concat([data['body_len'], data['punct%'], data['cap%'], pd.DataFrame(X_count_ps.toarray())], axis=1)\n",
    "\n",
    "# CountVectorizer using lemmatizing\n",
    "count_vect_wn = CountVectorizer(analyzer=clean_text_wn)\n",
    "X_count_wn = count_vect_wn.fit_transform(data['body_text'])\n",
    "X_count_feat_wn = pd.concat([data['body_len'], data['punct%'], data['cap%'], pd.DataFrame(X_count_wn.toarray())], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attributes**\n",
    "* `feature_importances_`: outputs the value of each feature to the model\n",
    "* `fit`: fit actual model and store that fit model as object\n",
    "* `predict`: make prediction on test set\n",
    "\n",
    "**Hyperparameters**\n",
    "* `max_depth`: the depth of each decision trees is, default is none\n",
    "* `n_estimators`: the number of decision trees built within random forest, default is 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier through Cross_Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF-IDF using stemming:  [0.97578475 0.97755835 0.97307002 0.96768402 0.97486535]\n",
      "TF-IDF using lemmatizing:  [0.96860987 0.97396768 0.97576302 0.95960503 0.97307002]\n",
      "Count using stemming:  [0.96860987 0.97307002 0.97576302 0.95960503 0.97576302]\n",
      "Count using lemmatizing:  [0.96950673 0.97037702 0.97486535 0.96140036 0.96947935]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Build individual decision trees in parallel\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "k_fold = KFold(n_splits=5)\n",
    "print('IF-IDF using stemming: ', cross_val_score(rf, X_tfidf_feat_ps, data['label'], cv=k_fold, scoring='accuracy', n_jobs=-1))\n",
    "print('TF-IDF using lemmatizing: ', cross_val_score(rf, X_tfidf_feat_wn, data['label'], cv=k_fold, scoring='accuracy', n_jobs=-1))\n",
    "print('Count using stemming: ', cross_val_score(rf, X_count_feat_ps, data['label'], cv=k_fold, scoring='accuracy', n_jobs=-1))\n",
    "print('Count using lemmatizing: ', cross_val_score(rf, X_count_feat_wn, data['label'], cv=k_fold, scoring='accuracy', n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier through Holdout Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.05895296291205725, 'body_len'),\n",
       " (0.046055213992170996, 'cap%'),\n",
       " (0.036850849709918206, 843),\n",
       " (0.03033015077545737, 1069),\n",
       " (0.029476376792644476, 3829)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features_ps, data['label'], test_size=0.2)\n",
    "\n",
    "rf_h = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1)\n",
    "rf_model = rf_h.fit(X_train, y_train)\n",
    "\n",
    "sorted(zip(rf_model.feature_importances_, X_train.columns), reverse=True)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 / Recall: 0.72 / Accuracy: 0.967\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf_model.predict(X_test)\n",
    "precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3),\n",
    "                                                        round(recall, 3),\n",
    "                                                        round((y_pred==y_test).sum()/len(y_test), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Precision** 100%: All mail in the spam folder is actually spam.\n",
    "* **Recall** 72%: 72% of the all spam that has come into email was properly placed in the spam folder.\n",
    "* **Accuracy** 96.7%: 96.7% of emails that have come into emails were correctly identified as spam or ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest model with Grid-search\n",
    "**Grid-search**: Exhaustively search all parameter combinations in a given grid to determine the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf(n_est, depth):\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1)\n",
    "    rf_model = rf.fit(X_train, y_train)\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print('Est: {} / Depth: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        n_est, depth, round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_test), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 10 / Depth: 10 ---- Precision: 1.0 / Recall: 0.303 / Accuracy: 0.917\n",
      "Est: 10 / Depth: 20 ---- Precision: 1.0 / Recall: 0.636 / Accuracy: 0.957\n",
      "Est: 10 / Depth: 30 ---- Precision: 1.0 / Recall: 0.803 / Accuracy: 0.977\n",
      "Est: 10 / Depth: None ---- Precision: 0.973 / Recall: 0.826 / Accuracy: 0.977\n",
      "Est: 50 / Depth: 10 ---- Precision: 1.0 / Recall: 0.371 / Accuracy: 0.926\n",
      "Est: 50 / Depth: 20 ---- Precision: 1.0 / Recall: 0.727 / Accuracy: 0.968\n",
      "Est: 50 / Depth: 30 ---- Precision: 1.0 / Recall: 0.795 / Accuracy: 0.976\n",
      "Est: 50 / Depth: None ---- Precision: 0.983 / Recall: 0.879 / Accuracy: 0.984\n",
      "Est: 100 / Depth: 10 ---- Precision: 1.0 / Recall: 0.318 / Accuracy: 0.919\n",
      "Est: 100 / Depth: 20 ---- Precision: 1.0 / Recall: 0.689 / Accuracy: 0.963\n",
      "Est: 100 / Depth: 30 ---- Precision: 1.0 / Recall: 0.803 / Accuracy: 0.977\n",
      "Est: 100 / Depth: None ---- Precision: 0.983 / Recall: 0.871 / Accuracy: 0.983\n"
     ]
    }
   ],
   "source": [
    "for n_est in [10, 50, 100]:\n",
    "    for depth in [10, 20, 30, None]:\n",
    "        train_rf(n_est, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through grid-search, we can see how certain parameters impact the aggressiveness in the model. Here, as the depth increases from 10, 20, to 30, and eventually to nono, the recall increases quite drastically, while the precision doesn't really drop. So the model is getting much better and more aggressive as the depth increases. On the other side, we noticed that adding estimators might be helping a little bit, but the improvement isn't as adding depth to the individual trees. 10 is clearly pretty bad, no matter how many estimators. 20 isn't really great either. Once get towards 30, it starts to level out. So we can eliminate model with limited max depth to get best random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Random Forest with GridSearchCV to explore parameter settings\n",
    "**Cross-validation**: Divide a dataset into *k* subsets and repeat the holdout method _k_ times where a different subset is used as the holdout set in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time:  514.623\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>34.836667</td>\n",
       "      <td>0.769263</td>\n",
       "      <td>0.415283</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 150}</td>\n",
       "      <td>0.979372</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.974865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976306</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>60.273258</td>\n",
       "      <td>6.539977</td>\n",
       "      <td>0.574894</td>\n",
       "      <td>0.073772</td>\n",
       "      <td>None</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 300}</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.976682</td>\n",
       "      <td>0.974865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975588</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>61.792665</td>\n",
       "      <td>0.592637</td>\n",
       "      <td>0.593537</td>\n",
       "      <td>0.016471</td>\n",
       "      <td>90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 300}</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975408</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.999641</td>\n",
       "      <td>0.000180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.960417</td>\n",
       "      <td>0.148955</td>\n",
       "      <td>0.225780</td>\n",
       "      <td>0.021746</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 10}</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.974888</td>\n",
       "      <td>0.974865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975049</td>\n",
       "      <td>0.004624</td>\n",
       "      <td>4</td>\n",
       "      <td>0.997756</td>\n",
       "      <td>0.998654</td>\n",
       "      <td>0.998654</td>\n",
       "      <td>0.998429</td>\n",
       "      <td>0.998205</td>\n",
       "      <td>0.998340</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32.746749</td>\n",
       "      <td>0.622644</td>\n",
       "      <td>0.402019</td>\n",
       "      <td>0.028170</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 150}</td>\n",
       "      <td>0.979372</td>\n",
       "      <td>0.973991</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975049</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.000246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "10      34.836667      0.769263         0.415283        0.030000   \n",
       "11      60.273258      6.539977         0.574894        0.073772   \n",
       "8       61.792665      0.592637         0.593537        0.016471   \n",
       "6        4.960417      0.148955         0.225780        0.021746   \n",
       "7       32.746749      0.622644         0.402019        0.028170   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "10            None                150   \n",
       "11            None                300   \n",
       "8               90                300   \n",
       "6               90                 10   \n",
       "7               90                150   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "10  {'max_depth': None, 'n_estimators': 150}           0.979372   \n",
       "11  {'max_depth': None, 'n_estimators': 300}           0.978475   \n",
       "8     {'max_depth': 90, 'n_estimators': 300}           0.978475   \n",
       "6      {'max_depth': 90, 'n_estimators': 10}           0.978475   \n",
       "7     {'max_depth': 90, 'n_estimators': 150}           0.979372   \n",
       "\n",
       "    split1_test_score  split2_test_score       ...         mean_test_score  \\\n",
       "10           0.977578           0.974865       ...                0.976306   \n",
       "11           0.976682           0.974865       ...                0.975588   \n",
       "8            0.975785           0.976661       ...                0.975408   \n",
       "6            0.974888           0.974865       ...                0.975049   \n",
       "7            0.973991           0.976661       ...                0.975049   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "10        0.001944                1            1.000000            1.000000   \n",
       "11        0.002086                2            1.000000            1.000000   \n",
       "8         0.002703                3            0.999551            0.999551   \n",
       "6         0.004624                4            0.997756            0.998654   \n",
       "7         0.003654                4            0.999327            0.999551   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "10            1.000000            1.000000            1.000000   \n",
       "11            1.000000            1.000000            1.000000   \n",
       "8             0.999551            1.000000            0.999551   \n",
       "6             0.998654            0.998429            0.998205   \n",
       "7             0.999327            1.000000            0.999551   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "10          1.000000         0.000000  \n",
       "11          1.000000         0.000000  \n",
       "8           0.999641         0.000180  \n",
       "6           0.998340         0.000336  \n",
       "7           0.999551         0.000246  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators': [10, 150, 300], 'max_depth': [30, 60, 90, None]}\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "# TF-IDF\n",
    "start = time.time()\n",
    "gs_fit = gs.fit(X_tfidf_feat_ps, data['label'])\n",
    "end = time.time()\n",
    "print('Fit time: ', round(end - start, 3))\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **mean_fit_time**: the average time it takes each model to fit\n",
    "* **mean_score_time**: the average amount of time it takes each model to make a prediction on the test set\n",
    "* **mean_test_score**: the average accuracy on the test set\n",
    "* **mean_train_score**: the average accuracy on the training set\n",
    "\n",
    "In terms of parameter combinations, we notice that the best performing models are the ones with the deepest individual decision trees. The number of estimators doesn't seem to matter as much. Looking at the mean_fit_time, it's much faster for 10 estimators than it is for 150 or 300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time:  502.035\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>34.628215</td>\n",
       "      <td>0.424908</td>\n",
       "      <td>0.435221</td>\n",
       "      <td>0.022161</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 150}</td>\n",
       "      <td>0.980269</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976126</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62.067042</td>\n",
       "      <td>0.487726</td>\n",
       "      <td>0.577043</td>\n",
       "      <td>0.023161</td>\n",
       "      <td>90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 300}</td>\n",
       "      <td>0.980269</td>\n",
       "      <td>0.976682</td>\n",
       "      <td>0.975763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975947</td>\n",
       "      <td>0.002868</td>\n",
       "      <td>2</td>\n",
       "      <td>0.998878</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.999103</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32.046166</td>\n",
       "      <td>0.326746</td>\n",
       "      <td>0.390415</td>\n",
       "      <td>0.015534</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 150}</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.976682</td>\n",
       "      <td>0.975763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975767</td>\n",
       "      <td>0.002413</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.999103</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.999372</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>61.821210</td>\n",
       "      <td>6.535141</td>\n",
       "      <td>0.596016</td>\n",
       "      <td>0.112341</td>\n",
       "      <td>None</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 300}</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.973968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975767</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.802290</td>\n",
       "      <td>0.212494</td>\n",
       "      <td>0.368870</td>\n",
       "      <td>0.033454</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 150}</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>0.972172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975049</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>5</td>\n",
       "      <td>0.995287</td>\n",
       "      <td>0.995287</td>\n",
       "      <td>0.996186</td>\n",
       "      <td>0.996635</td>\n",
       "      <td>0.995065</td>\n",
       "      <td>0.995692</td>\n",
       "      <td>0.000609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "10      34.628215      0.424908         0.435221        0.022161   \n",
       "8       62.067042      0.487726         0.577043        0.023161   \n",
       "7       32.046166      0.326746         0.390415        0.015534   \n",
       "11      61.821210      6.535141         0.596016        0.112341   \n",
       "4       26.802290      0.212494         0.368870        0.033454   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "10            None                150   \n",
       "8               90                300   \n",
       "7               90                150   \n",
       "11            None                300   \n",
       "4               60                150   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "10  {'max_depth': None, 'n_estimators': 150}           0.980269   \n",
       "8     {'max_depth': 90, 'n_estimators': 300}           0.980269   \n",
       "7     {'max_depth': 90, 'n_estimators': 150}           0.978475   \n",
       "11  {'max_depth': None, 'n_estimators': 300}           0.977578   \n",
       "4     {'max_depth': 60, 'n_estimators': 150}           0.978475   \n",
       "\n",
       "    split1_test_score  split2_test_score       ...         mean_test_score  \\\n",
       "10           0.975785           0.976661       ...                0.976126   \n",
       "8            0.976682           0.975763       ...                0.975947   \n",
       "7            0.976682           0.975763       ...                0.975767   \n",
       "11           0.977578           0.973968       ...                0.975767   \n",
       "4            0.975785           0.972172       ...                0.975049   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "10        0.002583                1            1.000000            1.000000   \n",
       "8         0.002868                2            0.998878            0.999327   \n",
       "7         0.002413                3            0.999327            0.999327   \n",
       "11        0.001615                3            1.000000            1.000000   \n",
       "4         0.002234                5            0.995287            0.995287   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "10            1.000000            1.000000            1.000000   \n",
       "8             0.999103            1.000000            0.999327   \n",
       "7             0.999103            0.999551            0.999551   \n",
       "11            1.000000            1.000000            1.000000   \n",
       "4             0.996186            0.996635            0.995065   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "10          1.000000         0.000000  \n",
       "8           0.999327         0.000375  \n",
       "7           0.999372         0.000168  \n",
       "11          1.000000         0.000000  \n",
       "4           0.995692         0.000609  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "start = time.time()\n",
    "gs_fit1 = gs.fit(X_count_feat_ps, data['label'])\n",
    "end = time.time()\n",
    "print('Fit time: ', round(end - start, 3))\n",
    "pd.DataFrame(gs_fit1.cv_results_).sort_values('mean_test_score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean_test_score at 97.61% for CountVectorizer is just a tight below 97.63% for TF-IDF, so TF-IDF is doing slightly better. The max_depth with 90 or none are in most of the top models on this side. The number of estimators seems that it matters a little bit more as there are only 150 and 300.\n",
    "## 6.4 Gradient Boosting model\n",
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall back to random forest, the default setting for `max_depth` is none, so it could build each tree as deep as it wanted. For gradient boosting, the `max_depth` default setting is three. Also for random forest the default for the number of estimators was 10, and for gradient boosting it's 100. Random forest is built with a couple fully grown trees, whereas gradient boosting uses a lot of very basic trees. We can notice there's no `n_jobs` paramter like for random forest. So gradient boosting can't be parallelized because each iteration builds on the prior iteration. `learning_rate` determines how quickly an algorithm optimizes, but it also has performance implications, because it can cause the model to optimize too quickly without truly finding the best model.\n",
    "### Gradiet Boosting model with Grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gb(est, max_depth, lr):\n",
    "    gb = GradientBoostingClassifier(n_estimators=est, max_depth=max_depth, learning_rate=lr)\n",
    "    start = time.time()\n",
    "    gb_model = gb.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    y_pred = gb_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print('Fit time: {} / Est: {} / Depth: {} / LR: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        round(end-start, 3), est, max_depth, lr, round(precision, 3), round(recall, 3), \n",
    "        round((y_pred==y_test).sum()/len(y_test), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 40.868 / Est: 50 / Depth: 3 / LR: 0.05 ---- Precision: 0.88 / Recall: 0.78 / Accuracy: 0.961\n",
      "Fit time: 38.697 / Est: 50 / Depth: 3 / LR: 0.1 ---- Precision: 0.889 / Recall: 0.848 / Accuracy: 0.97\n",
      "Fit time: 33.715 / Est: 50 / Depth: 3 / LR: 1 ---- Precision: 0.83 / Recall: 0.848 / Accuracy: 0.961\n",
      "Fit time: 97.537 / Est: 50 / Depth: 7 / LR: 0.05 ---- Precision: 0.884 / Recall: 0.864 / Accuracy: 0.97\n",
      "Fit time: 91.083 / Est: 50 / Depth: 7 / LR: 0.1 ---- Precision: 0.878 / Recall: 0.871 / Accuracy: 0.97\n",
      "Fit time: 72.068 / Est: 50 / Depth: 7 / LR: 1 ---- Precision: 0.898 / Recall: 0.871 / Accuracy: 0.973\n",
      "Fit time: 144.766 / Est: 50 / Depth: 11 / LR: 0.05 ---- Precision: 0.889 / Recall: 0.848 / Accuracy: 0.97\n",
      "Fit time: 141.948 / Est: 50 / Depth: 11 / LR: 0.1 ---- Precision: 0.885 / Recall: 0.871 / Accuracy: 0.971\n",
      "Fit time: 51.359 / Est: 50 / Depth: 11 / LR: 1 ---- Precision: 0.863 / Recall: 0.856 / Accuracy: 0.967\n",
      "Fit time: 188.081 / Est: 50 / Depth: 15 / LR: 0.05 ---- Precision: 0.897 / Recall: 0.856 / Accuracy: 0.971\n",
      "Fit time: 196.04 / Est: 50 / Depth: 15 / LR: 0.1 ---- Precision: 0.89 / Recall: 0.856 / Accuracy: 0.97\n",
      "Fit time: 61.073 / Est: 50 / Depth: 15 / LR: 1 ---- Precision: 0.885 / Recall: 0.871 / Accuracy: 0.971\n",
      "Fit time: 74.473 / Est: 100 / Depth: 3 / LR: 0.05 ---- Precision: 0.897 / Recall: 0.856 / Accuracy: 0.971\n",
      "Fit time: 69.966 / Est: 100 / Depth: 3 / LR: 0.1 ---- Precision: 0.958 / Recall: 0.856 / Accuracy: 0.978\n",
      "Fit time: 60.862 / Est: 100 / Depth: 3 / LR: 1 ---- Precision: 0.848 / Recall: 0.848 / Accuracy: 0.964\n",
      "Fit time: 177.534 / Est: 100 / Depth: 7 / LR: 0.05 ---- Precision: 0.891 / Recall: 0.871 / Accuracy: 0.972\n",
      "Fit time: 160.577 / Est: 100 / Depth: 7 / LR: 0.1 ---- Precision: 0.913 / Recall: 0.879 / Accuracy: 0.976\n",
      "Fit time: 81.545 / Est: 100 / Depth: 7 / LR: 1 ---- Precision: 0.899 / Recall: 0.879 / Accuracy: 0.974\n",
      "Fit time: 284.557 / Est: 100 / Depth: 11 / LR: 0.05 ---- Precision: 0.885 / Recall: 0.871 / Accuracy: 0.971\n",
      "Fit time: 255.602 / Est: 100 / Depth: 11 / LR: 0.1 ---- Precision: 0.891 / Recall: 0.871 / Accuracy: 0.972\n",
      "Fit time: 67.537 / Est: 100 / Depth: 11 / LR: 1 ---- Precision: 0.885 / Recall: 0.871 / Accuracy: 0.971\n",
      "Fit time: 385.259 / Est: 100 / Depth: 15 / LR: 0.05 ---- Precision: 0.89 / Recall: 0.856 / Accuracy: 0.97\n",
      "Fit time: 358.295 / Est: 100 / Depth: 15 / LR: 0.1 ---- Precision: 0.891 / Recall: 0.864 / Accuracy: 0.971\n",
      "Fit time: 58.096 / Est: 100 / Depth: 15 / LR: 1 ---- Precision: 0.878 / Recall: 0.871 / Accuracy: 0.97\n",
      "Fit time: 105.793 / Est: 150 / Depth: 3 / LR: 0.05 ---- Precision: 0.95 / Recall: 0.864 / Accuracy: 0.978\n",
      "Fit time: 99.903 / Est: 150 / Depth: 3 / LR: 0.1 ---- Precision: 0.958 / Recall: 0.864 / Accuracy: 0.979\n",
      "Fit time: 87.989 / Est: 150 / Depth: 3 / LR: 1 ---- Precision: 0.857 / Recall: 0.864 / Accuracy: 0.967\n",
      "Fit time: 597.182 / Est: 150 / Depth: 7 / LR: 0.05 ---- Precision: 0.906 / Recall: 0.879 / Accuracy: 0.975\n",
      "Fit time: 220.379 / Est: 150 / Depth: 7 / LR: 0.1 ---- Precision: 0.906 / Recall: 0.879 / Accuracy: 0.975\n",
      "Fit time: 81.835 / Est: 150 / Depth: 7 / LR: 1 ---- Precision: 0.884 / Recall: 0.864 / Accuracy: 0.97\n",
      "Fit time: 393.699 / Est: 150 / Depth: 11 / LR: 0.05 ---- Precision: 0.885 / Recall: 0.879 / Accuracy: 0.972\n",
      "Fit time: 363.073 / Est: 150 / Depth: 11 / LR: 0.1 ---- Precision: 0.899 / Recall: 0.879 / Accuracy: 0.974\n",
      "Fit time: 48.342 / Est: 150 / Depth: 11 / LR: 1 ---- Precision: 0.877 / Recall: 0.864 / Accuracy: 0.97\n",
      "Fit time: 561.755 / Est: 150 / Depth: 15 / LR: 0.05 ---- Precision: 0.89 / Recall: 0.856 / Accuracy: 0.97\n",
      "Fit time: 511.624 / Est: 150 / Depth: 15 / LR: 0.1 ---- Precision: 0.891 / Recall: 0.871 / Accuracy: 0.972\n",
      "Fit time: 61.083 / Est: 150 / Depth: 15 / LR: 1 ---- Precision: 0.877 / Recall: 0.864 / Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "for n_est in [50, 100, 150]:\n",
    "    for max_depth in [3, 7, 11, 15]:\n",
    "        for lr in [0.05, 0.1, 1]:\n",
    "            train_gb(n_est, max_depth, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If set the learning rate too low, the models might not be able to predict any messages to be spam, the precision can't be calculated and will be set to zero.\n",
    "\n",
    "**Poorly performing models**\n",
    "* Very low max depth\n",
    "* Very low number of estimators\n",
    "\n",
    "**Best performing models**\n",
    "* Learning rate of 0.1\n",
    "* Very high max depth\n",
    "* Very high number of estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Gradient Boosting with GridSearchCV to explore parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time:  54150.29\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>522.031952</td>\n",
       "      <td>11.573965</td>\n",
       "      <td>0.379298</td>\n",
       "      <td>0.042388</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.3, 'max_depth': 11, 'n_est...</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>0.983857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977383</td>\n",
       "      <td>0.003834</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>350.827729</td>\n",
       "      <td>3.627007</td>\n",
       "      <td>0.352834</td>\n",
       "      <td>0.028580</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.3, 'max_depth': 7, 'n_esti...</td>\n",
       "      <td>0.976682</td>\n",
       "      <td>0.982960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977203</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>412.563420</td>\n",
       "      <td>25.534372</td>\n",
       "      <td>0.312962</td>\n",
       "      <td>0.010733</td>\n",
       "      <td>0.3</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.3, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>0.984753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977203</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>432.301239</td>\n",
       "      <td>16.106874</td>\n",
       "      <td>0.347346</td>\n",
       "      <td>0.013523</td>\n",
       "      <td>0.3</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.3, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.976682</td>\n",
       "      <td>0.985650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977024</td>\n",
       "      <td>0.004780</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>373.915833</td>\n",
       "      <td>2.729820</td>\n",
       "      <td>0.343288</td>\n",
       "      <td>0.026163</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.3, 'max_depth': 11, 'n_est...</td>\n",
       "      <td>0.976682</td>\n",
       "      <td>0.982960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976485</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "9      522.031952     11.573965         0.379298        0.042388   \n",
       "7      350.827729      3.627007         0.352834        0.028580   \n",
       "11     412.563420     25.534372         0.312962        0.010733   \n",
       "10     432.301239     16.106874         0.347346        0.013523   \n",
       "8      373.915833      2.729820         0.343288        0.026163   \n",
       "\n",
       "   param_learning_rate param_max_depth param_n_estimators  \\\n",
       "9                  0.3              11                150   \n",
       "7                  0.3               7                150   \n",
       "11                 0.3              15                150   \n",
       "10                 0.3              15                100   \n",
       "8                  0.3              11                100   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "9   {'learning_rate': 0.3, 'max_depth': 11, 'n_est...           0.975785   \n",
       "7   {'learning_rate': 0.3, 'max_depth': 7, 'n_esti...           0.976682   \n",
       "11  {'learning_rate': 0.3, 'max_depth': 15, 'n_est...           0.975785   \n",
       "10  {'learning_rate': 0.3, 'max_depth': 15, 'n_est...           0.976682   \n",
       "8   {'learning_rate': 0.3, 'max_depth': 11, 'n_est...           0.976682   \n",
       "\n",
       "    split1_test_score       ...         mean_test_score  std_test_score  \\\n",
       "9            0.983857       ...                0.977383        0.003834   \n",
       "7            0.982960       ...                0.977203        0.003253   \n",
       "11           0.984753       ...                0.977203        0.004573   \n",
       "10           0.985650       ...                0.977024        0.004780   \n",
       "8            0.982960       ...                0.976485        0.003921   \n",
       "\n",
       "    rank_test_score  split0_train_score  split1_train_score  \\\n",
       "9                 1                 1.0                 1.0   \n",
       "7                 2                 1.0                 1.0   \n",
       "11                2                 1.0                 1.0   \n",
       "10                4                 1.0                 1.0   \n",
       "8                 5                 1.0                 1.0   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "9                  1.0                 1.0                 1.0   \n",
       "7                  1.0                 1.0                 1.0   \n",
       "11                 1.0                 1.0                 1.0   \n",
       "10                 1.0                 1.0                 1.0   \n",
       "8                  1.0                 1.0                 1.0   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "9                1.0              0.0  \n",
       "7                1.0              0.0  \n",
       "11               1.0              0.0  \n",
       "10               1.0              0.0  \n",
       "8                1.0              0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "param = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [7, 11, 15],\n",
    "    'learning_rate': [0.1, 0.3]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(gb, param, cv=5, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "# TF-IDF\n",
    "start = time.time()\n",
    "gs_fit = gs.fit(X_tfidf_feat_ps, data['label'])\n",
    "end = time.time()\n",
    "print('Fit time: ', round(end - start, 3))\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookint at results for TF-IDF, in `mean_fit_time`, for random forest, the most time consuming model took around 60 secondes to fit. Gradient boosting models are taking a minimum of 350 seconds, and up to almost 500. Move to `mean_train_score`, these models are getting perfect scores on the training set. If the model is overfitting, to the point of just memorizing the training set, then that's bad because it won't do well generalizing to the test set. But if the model is just getting so good that it's doing really well on the training set, and it can still generalize, then that's great. The `mean_test_score` tells us whether the model can generalize to data that it was not trained on. The `mean_test_score` are all just around 97.77%, or just below, and the best models are the ones with 150 estimators, and around 11 `max_depth`. So the very best model has 150 estimators, and a `max_depth` of 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time:  65641.601\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>339.770966</td>\n",
       "      <td>3.352380</td>\n",
       "      <td>0.339256</td>\n",
       "      <td>0.021179</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.3, 'max_depth': 7, 'n_esti...</td>\n",
       "      <td>0.980269</td>\n",
       "      <td>0.986547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978639</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2311.477935</td>\n",
       "      <td>2177.338841</td>\n",
       "      <td>0.330816</td>\n",
       "      <td>0.043589</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.3, 'max_depth': 11, 'n_est...</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.986547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978101</td>\n",
       "      <td>0.005592</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2206.522328</td>\n",
       "      <td>2181.255303</td>\n",
       "      <td>0.345557</td>\n",
       "      <td>0.024413</td>\n",
       "      <td>0.3</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.3, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>0.986547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977383</td>\n",
       "      <td>0.005491</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>421.516874</td>\n",
       "      <td>20.547864</td>\n",
       "      <td>0.322416</td>\n",
       "      <td>0.011674</td>\n",
       "      <td>0.3</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.3, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>0.985650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977203</td>\n",
       "      <td>0.005290</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>232.196359</td>\n",
       "      <td>6.406001</td>\n",
       "      <td>0.336145</td>\n",
       "      <td>0.034254</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.3, 'max_depth': 7, 'n_esti...</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.984753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977024</td>\n",
       "      <td>0.005561</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "7      339.770966      3.352380         0.339256        0.021179   \n",
       "9     2311.477935   2177.338841         0.330816        0.043589   \n",
       "10    2206.522328   2181.255303         0.345557        0.024413   \n",
       "11     421.516874     20.547864         0.322416        0.011674   \n",
       "6      232.196359      6.406001         0.336145        0.034254   \n",
       "\n",
       "   param_learning_rate param_max_depth param_n_estimators  \\\n",
       "7                  0.3               7                150   \n",
       "9                  0.3              11                150   \n",
       "10                 0.3              15                100   \n",
       "11                 0.3              15                150   \n",
       "6                  0.3               7                100   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "7   {'learning_rate': 0.3, 'max_depth': 7, 'n_esti...           0.980269   \n",
       "9   {'learning_rate': 0.3, 'max_depth': 11, 'n_est...           0.978475   \n",
       "10  {'learning_rate': 0.3, 'max_depth': 15, 'n_est...           0.975785   \n",
       "11  {'learning_rate': 0.3, 'max_depth': 15, 'n_est...           0.975785   \n",
       "6   {'learning_rate': 0.3, 'max_depth': 7, 'n_esti...           0.977578   \n",
       "\n",
       "    split1_test_score       ...         mean_test_score  std_test_score  \\\n",
       "7            0.986547       ...                0.978639        0.005784   \n",
       "9            0.986547       ...                0.978101        0.005592   \n",
       "10           0.986547       ...                0.977383        0.005491   \n",
       "11           0.985650       ...                0.977203        0.005290   \n",
       "6            0.984753       ...                0.977024        0.005561   \n",
       "\n",
       "    rank_test_score  split0_train_score  split1_train_score  \\\n",
       "7                 1                 1.0                 1.0   \n",
       "9                 2                 1.0                 1.0   \n",
       "10                3                 1.0                 1.0   \n",
       "11                4                 1.0                 1.0   \n",
       "6                 5                 1.0                 1.0   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "7                  1.0                 1.0                 1.0   \n",
       "9                  1.0                 1.0                 1.0   \n",
       "10                 1.0                 1.0                 1.0   \n",
       "11                 1.0                 1.0                 1.0   \n",
       "6                  1.0                 1.0                 1.0   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "7                1.0              0.0  \n",
       "9                1.0              0.0  \n",
       "10               1.0              0.0  \n",
       "11               1.0              0.0  \n",
       "6                1.0              0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "start = time.time()\n",
    "gs_fit1 = gs.fit(X_count_feat_ps, data['label'])\n",
    "end = time.time()\n",
    "print('Fit time: ', round(end - start, 3))\n",
    "pd.DataFrame(gs_fit1.cv_results_).sort_values('mean_test_score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results for the count vectorizing, the time it takes to fit the `mean_train_score`, and `mean_test_score`, are quite different, some are all pretty much right in line with TF-IDF, some are not. However, there's some difference on `mean_test_score`, all the test score results are just above 97.7%, and the best model reaches 97.8%. It's the one with 150 estimators, and a `max_depth` of 7. Thus, count vectorizing performs a little better than TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('SMSSpamCollection', sep='\\t')\n",
    "data1.columns = ['label', 'body_text']\n",
    "\n",
    "data1['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(' '))\n",
    "data1['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "data1['cap%'] = data['body_text'].apply(lambda x: count_cap(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data1[['body_text', 'body_len', 'punct%', 'cap%']],\n",
    "                                                    data1['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>cap%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>6347</th>\n",
       "      <th>6348</th>\n",
       "      <th>6349</th>\n",
       "      <th>6350</th>\n",
       "      <th>6351</th>\n",
       "      <th>6352</th>\n",
       "      <th>6353</th>\n",
       "      <th>6354</th>\n",
       "      <th>6355</th>\n",
       "      <th>6356</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>129</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156</td>\n",
       "      <td>10.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>5.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6360 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%  cap%    0    1    2    3    4    5    6  ...   6347  \\\n",
       "0       129     4.7   8.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "1       156    10.9   3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "2       124     3.2   5.6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "3       128     7.0  18.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "4        36     5.6   5.6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "\n",
       "   6348  6349  6350  6351  6352  6353  6354  6355  6356  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 6360 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text_ps)\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])\n",
    "\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n",
    "\n",
    "X_train_vect = pd.concat([X_train[['body_len', 'punct%', 'cap%']].reset_index(drop=True),\n",
    "                          pd.DataFrame(tfidf_train.toarray())], axis=1)\n",
    "X_test_vect = pd.concat([X_test[['body_len', 'punct%', 'cap%']].reset_index(drop=True),\n",
    "                          pd.DataFrame(tfidf_test.toarray())], axis=1)\n",
    "\n",
    "X_train_vect.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final evaluation of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 8.315 / Predict time: 0.223 ---- Precision: 1.0 / Recall: 0.853 / Accuracy: 0.982\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = end - start\n",
    "\n",
    "start = time.time()\n",
    "y_pred = rf_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = end - start\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3),\n",
    "    round((y_pred==y_test).sum()/len(y_test), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 338.49 / Predict time: 0.208 ---- Precision: 0.984 / Recall: 0.912 / Accuracy: 0.987\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=11, learning_rate=0.3)\n",
    "\n",
    "start = time.time()\n",
    "gb_model = gb.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = end - start\n",
    "\n",
    "start = time.time()\n",
    "y_pred = gb_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = end -start\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3),\n",
    "    round((y_test==y_pred).sum()/len(y_test), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With particular focus on predict time, precision, and recall, let's do a final comparison. Once these models are fit, they wouldn't really ever be refit or retrained again until current model needs to be replaced. Thus we care more about predict time than fit time. Moreover, precision and recall can give a more in-depth look into how is the model's performence. Even though gradient boosting taks way longer than random forest does to fit, it actually takes less time to predict. In terms of precision and recall, random forest has much better precision at 100%, but gradient boosting has slightly better recall. So there will be a trade off when we pick up model. If we care more about precision than than we do predict time or recall, we can use random forest, and vice versa.\n",
    "\n",
    "Further evaluation: Slice test set in a variety of different ways to understand how it does across a number of different dimensions, s.t. text meassages that have a length greater than 50 or that have zero punctuation. Examine text messages the model is getting wrong.\n",
    "\n",
    "Results of trade-off &mdash; consider business context: Is predict time going to create a bottlenect? Spam filter &mdash; optimize for precision. Antivirus software &mdash; optimize for recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
